{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my First Simple Model I set up just one Conv2D/Pooling Couplet to see what kind of results I would get. I will be using this as my baseline. This step really informed my iterative process because its poor results made me reconsider my data preparation. I intially hadn't cropped the still images taken from the videos to be of the face but after seeing the results of this model I went back and procesed the images again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "keIiLw3fTC5o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models, Sequential\n",
    "from tensorflow.keras import models, layers, optimizers, regularizers\n",
    "# from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RWDyqD4bicEP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19862 files belonging to 2 classes.\n",
      "Found 6208 files belonging to 2 classes.\n",
      "Found 4965 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = tf.keras.utils.image_dataset_from_directory('/Users/jordanmang/Flatiron/Flatiron_Labs/FaceForensics/Data_Split/train')\n",
    "test = tf.keras.utils.image_dataset_from_directory('/Users/jordanmang/Flatiron/Flatiron_Labs/FaceForensics/Data_Split/test')\n",
    "val = tf.keras.utils.image_dataset_from_directory('/Users/jordanmang/Flatiron/Flatiron_Labs/FaceForensics/Data_Split/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VoXG5p2gmF9"
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4M7xs3z2gsUd"
   },
   "outputs": [],
   "source": [
    "# Add layers to model\n",
    "# input layer\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu', padding = 'same', input_shape=(256,256,3)))\n",
    "\n",
    "# add pooling layer(takes max from input window)\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# flattens 2d to 1d\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# add dense layer\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "\n",
    "# add output layer\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Et9G9w0Xgv-P"
   },
   "outputs": [],
   "source": [
    "# compile model with adam for binary model\n",
    "model.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-wwjiqqg1Uv",
    "outputId": "c532a479-9c2b-4d1a-e624-597985915d0a"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(train,\n",
    "               batch_size=50,\n",
    "               epochs=10,\n",
    "               validation_data=(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nkHzU6Lg5NI"
   },
   "outputs": [],
   "source": [
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSDjYwskg_ly"
   },
   "outputs": [],
   "source": [
    "# Get precision, recall, and accuracy for test batch set\n",
    "for batch in test.as_numpy_iterator(): \n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    pre.update_state(y, yhat)\n",
    "    re.update_state(y, yhat)\n",
    "    acc.update_state(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlH66KMgzxAG",
    "outputId": "643a8686-af3b-4baa-8033-7389ba5103bc"
   },
   "outputs": [],
   "source": [
    "print(f'Precision: {pre.result()}, Recall: {re.result()}, Accuracy: {acc.result()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of FSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 1 (OG DATA): Precision: 0.5370051860809326, Recall: 0.0812288448214531, Accuracy: 0.5050138235092163\n",
    "<br>MODEL 1 (V2 DATA): Precision: 0.5091816782951355, Recall: 1.0, Accuracy: 0.5091816782951355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both sets of data, including the new data, didn't seem to do so well with the first simple model. I believe this to be that because of the simplicity it is having a hard time with such a difficult set of images. For the second model I will be increasing the complexity in hopes of getting past this barrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AN4XBNQ0ARmH"
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExVAyQFHAWCP"
   },
   "outputs": [],
   "source": [
    "# Add layers to model\n",
    "# input layer\n",
    "model2.add(Conv2D(64, (3,3), 1, activation='relu', padding = 'same', input_shape=(256,256,3)))\n",
    "model2.add(Conv2D(64, (3,3)))\n",
    "model2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# flattens 2d to 1d\n",
    "model2.add(layers.Flatten())\n",
    "\n",
    "# add dense layer\n",
    "model2.add(layers.Dense(16, activation='relu'))\n",
    "\n",
    "# add output layer\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKxN8ODKAd_y"
   },
   "outputs": [],
   "source": [
    "# compile model with adam for binary model\n",
    "model2.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "oGJLuGJcAlo-",
    "outputId": "0513ea38-8d2e-4e7d-880d-e47e59811a8a"
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model2.fit(train,\n",
    "               batch_size=50,\n",
    "               epochs=10,\n",
    "               validation_data=(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 2 (V2 DATA): Precision: 0.5091816782951355, Recall: 1.0, Accuracy: 0.5091816782951355"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know, I really thought that this was going to help out but all it really did was get it to reach its cap a little faster! I will trya again to increase the complexity because I still believe that the reason we are not seeing an increase in accuracy is because the model doesn't have ewnough neurons to train. This is just a feeling but with that being said I think that increasing the Dense layers at the end may help get this thing going!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model3 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers to model\n",
    "# input layer\n",
    "model3.add(Conv2D(64, (3,3), 1, activation='relu', padding = 'same', input_shape=(256,256,3)))\n",
    "model3.add(Conv2D(64, (3,3)))\n",
    "model3.add(layers.MaxPooling2D((2,2)))\n",
    "# flattens 2d to 1d\n",
    "model3.add(layers.Flatten())\n",
    "\n",
    "# add dense layer\n",
    "model3.add(Dense(4096, activation = 'relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "# add output layer\n",
    "model3.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with adam for binary model\n",
    "model3.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model3.fit(train,\n",
    "               batch_size=50,\n",
    "               epochs=10,\n",
    "               validation_data=(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 3 (V2 DATA): Precision: 0.8682048320770264, Recall: 0.8690288066864014, Accuracy: 0.8661404848098755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES! It seems like I was right! The only thing that is starting to concern me now is the thought that maybe my first set of data wasnt all that bad? Maybe my neural network was just trash.. But.. we are a bit too far into this now to change things up and my gut is telling me that I needed to crop the images down to the face! For the next model I will probably try adding another dense layer and drop out layer and we can see what kind of results we get!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model4 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add layers to model\n",
    "# input layer\n",
    "model4.add(Conv2D(64, (3,3), 1, activation='relu', padding = 'same', input_shape=(256,256,3)))\n",
    "model4.add(Conv2D(64, (3,3)))\n",
    "model4.add(layers.MaxPooling2D((2,2)))\n",
    "# flattens 2d to 1d\n",
    "model4.add(layers.Flatten())\n",
    "\n",
    "# add dense layer\n",
    "model4.add(Dense(4096, activation = 'relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "model4.add(Dense(4096, activation = 'relu'))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "# add output layer\n",
    "model4.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with adam for binary model\n",
    "model4.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model4.fit(train,\n",
    "               batch_size=50,\n",
    "               epochs=10,\n",
    "               validation_data=(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Model 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL 4 (V2 DATA): Precision: 0.8511613607406616, Recall: 0.8810502886772156, Accuracy: 0.8609858155250549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmm it seems to have only increase the Recall and tthat isn't really all that important to us. I wonder if adding more Conv2 layers will help at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model5 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "model5.add(Conv2D(64, (3,3), 1, activation='relu', padding = 'same', input_shape=(256,256,3)))\n",
    "model5.add(Conv2D(64, (3,3)))\n",
    "model5.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "model5.add(Conv2D(filters = 128, kernel_size = (3,3)))\n",
    "model5.add(Conv2D(filters = 128, kernel_size = (3,3)))\n",
    "model5.add(MaxPooling2D(pool_size = (2,2), strides = 2))\n",
    "\n",
    "# flattens 2d to 1d\n",
    "model5.add(layers.Flatten())\n",
    "\n",
    "# add dense layer\n",
    "model5.add(Dense(4096, activation = 'relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "model5.add(Dense(4096, activation = 'relu'))\n",
    "model5.add(Dropout(0.5))\n",
    "\n",
    "# add output layer\n",
    "model5.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model with adam for binary model\n",
    "model5.compile('adam', loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model5.fit(train,\n",
    "               batch_size=10,\n",
    "               epochs=10,\n",
    "               validation_data=(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Model 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Aways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "well It seems that I have reached the bounds of both my computer and Google CoLab with this model. But to be honest, thats okay because I have recently found out that VGG is a better version of the model that I was building and it will run better anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG models will begin in the VGG notebooks"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
