{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import os.path\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = r'/Users/jordanmang/Flatiron/Flatiron_Labs/FaceForensics/Capstone_GH/ORIGINAL'\n",
    "out = r'/Users/jordanmang/Flatiron/Flatiron_Labs/FaceForensics/Capstone_GH/ORIGINALFACES/'\n",
    "\n",
    "for file in os.listdir(f):\n",
    "    f_img = f + '/' + file\n",
    "    out_img = out + '/' + file\n",
    "    img = cv2.imread(f_img)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_alt2.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.2, 4, minSize=(30, 30))\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "        faces = img[y:y + h, x:x + w]\n",
    "        cv2.imwrite(out_img, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is pretty straight forward, the variables on top are where your images are stored and where you would like them to go. The firs tline of code uses OS commands to loop through images in a directory, finds faces in them and then saves a cropped version of just the face."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
